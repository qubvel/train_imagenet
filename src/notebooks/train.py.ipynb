{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "sys.path.append('..')\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset import ImageNetDataset\n",
    "from utils.transform import train_transform, valid_transform\n",
    "from utils.generator import generator\n",
    "from utils.callbacks import get_callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels mapping\n",
    "with open(config.MAP_CLS) as f:\n",
    "    label_to_class = json.load(f)  \n",
    "folder_to_label = {v[0]: k for k, v in label_to_class.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "train_dataset = ImageNetDataset(config.TRAIN_DIR, folder_to_label, transform=train_transform)\n",
    "valid_dataset = ImageNetDataset(config.VALID_DIR, folder_to_label, transform=valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare generators \n",
    "train_gen = generator(train_dataset, batch_size=256, num_workers=12, shuffle=True)\n",
    "valid_gen = generator(valid_dataset, batch_size=1, num_workers=12, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nn_models.dpn import DPN92\n",
    "from nn_models.se_models.se_resnet import SEResNet18, SEResNet34\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare model\n",
    "\n",
    "model = SEResNet34(input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = multi_gpu_model(model, gpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.load_weights('../../checkpoints/se_resnet18/weights_ep210.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = SGD(lr=0.6, momentum=0.9)\n",
    "model.compile(opt, 'categorical_crossentropy', \n",
    "              ['categorical_accuracy', 'top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "callbacks = get_callbacks('se_resnet34', \n",
    "                          checkpoints_dir=config.CHECKPOINTS_DIR, \n",
    "                          monitor='val_loss',\n",
    "                          log_dir=config.LOGS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "K.set_value(model.optimizer.lr, 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_callbacks = callbacks[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2603s 521ms/step - loss: 5.2210 - categorical_accuracy: 0.1157 - top_k_categorical_accuracy: 0.2685 - val_loss: 4.5846 - val_categorical_accuracy: 0.1897 - val_top_k_categorical_accuracy: 0.4023\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.58462, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.1897.h5\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2590s 518ms/step - loss: 4.0190 - categorical_accuracy: 0.2666 - top_k_categorical_accuracy: 0.5115 - val_loss: 4.0515 - val_categorical_accuracy: 0.2635 - val_top_k_categorical_accuracy: 0.5176\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.58462 to 4.05150, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.2635.h5\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2600s 520ms/step - loss: 3.5931 - categorical_accuracy: 0.3341 - top_k_categorical_accuracy: 0.5938 - val_loss: 3.8016 - val_categorical_accuracy: 0.3071 - val_top_k_categorical_accuracy: 0.5623\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.05150 to 3.80157, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.3071.h5\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2618s 524ms/step - loss: 3.3477 - categorical_accuracy: 0.3745 - top_k_categorical_accuracy: 0.6374 - val_loss: 3.2869 - val_categorical_accuracy: 0.3886 - val_top_k_categorical_accuracy: 0.6543\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.80157 to 3.28695, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.3886.h5\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2591s 518ms/step - loss: 3.1839 - categorical_accuracy: 0.4034 - top_k_categorical_accuracy: 0.6653 - val_loss: 3.4051 - val_categorical_accuracy: 0.3650 - val_top_k_categorical_accuracy: 0.6292\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2595s 519ms/step - loss: 3.0608 - categorical_accuracy: 0.4239 - top_k_categorical_accuracy: 0.6864 - val_loss: 3.2482 - val_categorical_accuracy: 0.3866 - val_top_k_categorical_accuracy: 0.6572\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.28695 to 3.24823, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.3866.h5\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2597s 519ms/step - loss: 2.9602 - categorical_accuracy: 0.4419 - top_k_categorical_accuracy: 0.7025 - val_loss: 3.0921 - val_categorical_accuracy: 0.4195 - val_top_k_categorical_accuracy: 0.6851\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.24823 to 3.09206, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.4195.h5\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2589s 518ms/step - loss: 2.8844 - categorical_accuracy: 0.4552 - top_k_categorical_accuracy: 0.7145 - val_loss: 2.9136 - val_categorical_accuracy: 0.4474 - val_top_k_categorical_accuracy: 0.7138\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.09206 to 2.91362, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.4474.h5\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2588s 518ms/step - loss: 2.8134 - categorical_accuracy: 0.4686 - top_k_categorical_accuracy: 0.7251 - val_loss: 2.8865 - val_categorical_accuracy: 0.4483 - val_top_k_categorical_accuracy: 0.7172\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.91362 to 2.88651, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.4483.h5\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2590s 518ms/step - loss: 2.7549 - categorical_accuracy: 0.4787 - top_k_categorical_accuracy: 0.7347 - val_loss: 3.2985 - val_categorical_accuracy: 0.3900 - val_top_k_categorical_accuracy: 0.6447\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2589s 518ms/step - loss: 2.7041 - categorical_accuracy: 0.4886 - top_k_categorical_accuracy: 0.7420 - val_loss: 2.9497 - val_categorical_accuracy: 0.4442 - val_top_k_categorical_accuracy: 0.7069\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2588s 518ms/step - loss: 2.6572 - categorical_accuracy: 0.4968 - top_k_categorical_accuracy: 0.7494 - val_loss: 2.7231 - val_categorical_accuracy: 0.4806 - val_top_k_categorical_accuracy: 0.7428\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.88651 to 2.72313, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.4806.h5\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2586s 517ms/step - loss: 2.6144 - categorical_accuracy: 0.5045 - top_k_categorical_accuracy: 0.7558 - val_loss: 2.9293 - val_categorical_accuracy: 0.4508 - val_top_k_categorical_accuracy: 0.7084\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2589s 518ms/step - loss: 2.5799 - categorical_accuracy: 0.5115 - top_k_categorical_accuracy: 0.7611 - val_loss: 2.6213 - val_categorical_accuracy: 0.4995 - val_top_k_categorical_accuracy: 0.7587\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.72313 to 2.62125, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.4995.h5\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2591s 518ms/step - loss: 2.5442 - categorical_accuracy: 0.5183 - top_k_categorical_accuracy: 0.7656 - val_loss: 2.9245 - val_categorical_accuracy: 0.4521 - val_top_k_categorical_accuracy: 0.7174\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2593s 519ms/step - loss: 2.5166 - categorical_accuracy: 0.5238 - top_k_categorical_accuracy: 0.7701 - val_loss: 2.6636 - val_categorical_accuracy: 0.4915 - val_top_k_categorical_accuracy: 0.7517\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2593s 519ms/step - loss: 2.4844 - categorical_accuracy: 0.5291 - top_k_categorical_accuracy: 0.7744 - val_loss: 2.6116 - val_categorical_accuracy: 0.5061 - val_top_k_categorical_accuracy: 0.7643\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.62125 to 2.61157, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.5061.h5\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2594s 519ms/step - loss: 2.4602 - categorical_accuracy: 0.5336 - top_k_categorical_accuracy: 0.7775 - val_loss: 2.6148 - val_categorical_accuracy: 0.5060 - val_top_k_categorical_accuracy: 0.7569\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2594s 519ms/step - loss: 2.4348 - categorical_accuracy: 0.5388 - top_k_categorical_accuracy: 0.7814 - val_loss: 2.4419 - val_categorical_accuracy: 0.5374 - val_top_k_categorical_accuracy: 0.7866\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.61157 to 2.44193, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.5374.h5\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 2581s 516ms/step - loss: 2.4105 - categorical_accuracy: 0.5437 - top_k_categorical_accuracy: 0.7848 - val_loss: 2.7403 - val_categorical_accuracy: 0.4886 - val_top_k_categorical_accuracy: 0.7411\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2580s 516ms/step - loss: 2.3920 - categorical_accuracy: 0.5472 - top_k_categorical_accuracy: 0.7875 - val_loss: 2.5978 - val_categorical_accuracy: 0.5154 - val_top_k_categorical_accuracy: 0.7599\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2581s 516ms/step - loss: 2.3694 - categorical_accuracy: 0.5514 - top_k_categorical_accuracy: 0.7907 - val_loss: 2.4949 - val_categorical_accuracy: 0.5277 - val_top_k_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2582s 516ms/step - loss: 2.3500 - categorical_accuracy: 0.5552 - top_k_categorical_accuracy: 0.7935 - val_loss: 3.4648 - val_categorical_accuracy: 0.3882 - val_top_k_categorical_accuracy: 0.6283\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2583s 517ms/step - loss: 2.3330 - categorical_accuracy: 0.5586 - top_k_categorical_accuracy: 0.7960 - val_loss: 2.7156 - val_categorical_accuracy: 0.4906 - val_top_k_categorical_accuracy: 0.7412\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 2.3167 - categorical_accuracy: 0.5617 - top_k_categorical_accuracy: 0.7981 - val_loss: 2.6525 - val_categorical_accuracy: 0.5039 - val_top_k_categorical_accuracy: 0.7572\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2589s 518ms/step - loss: 2.3075 - categorical_accuracy: 0.5639 - top_k_categorical_accuracy: 0.7994 - val_loss: 2.4039 - val_categorical_accuracy: 0.5499 - val_top_k_categorical_accuracy: 0.7915\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.44193 to 2.40386, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.5499.h5\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2591s 518ms/step - loss: 2.2897 - categorical_accuracy: 0.5668 - top_k_categorical_accuracy: 0.8018 - val_loss: 2.4554 - val_categorical_accuracy: 0.5347 - val_top_k_categorical_accuracy: 0.7843\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2590s 518ms/step - loss: 2.2767 - categorical_accuracy: 0.5694 - top_k_categorical_accuracy: 0.8034 - val_loss: 2.7434 - val_categorical_accuracy: 0.4934 - val_top_k_categorical_accuracy: 0.7390\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2589s 518ms/step - loss: 2.2584 - categorical_accuracy: 0.5735 - top_k_categorical_accuracy: 0.8060 - val_loss: 2.5560 - val_categorical_accuracy: 0.5195 - val_top_k_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2588s 518ms/step - loss: 2.2457 - categorical_accuracy: 0.5757 - top_k_categorical_accuracy: 0.8076 - val_loss: 2.5634 - val_categorical_accuracy: 0.5121 - val_top_k_categorical_accuracy: 0.7695\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2586s 517ms/step - loss: 2.2330 - categorical_accuracy: 0.5781 - top_k_categorical_accuracy: 0.8095 - val_loss: 2.5152 - val_categorical_accuracy: 0.5301 - val_top_k_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 2.2228 - categorical_accuracy: 0.5801 - top_k_categorical_accuracy: 0.8110 - val_loss: 3.1597 - val_categorical_accuracy: 0.4335 - val_top_k_categorical_accuracy: 0.6710\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2589s 518ms/step - loss: 2.2073 - categorical_accuracy: 0.5831 - top_k_categorical_accuracy: 0.8128 - val_loss: 2.8888 - val_categorical_accuracy: 0.4717 - val_top_k_categorical_accuracy: 0.7145\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 2.1979 - categorical_accuracy: 0.5852 - top_k_categorical_accuracy: 0.8144 - val_loss: 2.5457 - val_categorical_accuracy: 0.5270 - val_top_k_categorical_accuracy: 0.7649\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2588s 518ms/step - loss: 2.1843 - categorical_accuracy: 0.5882 - top_k_categorical_accuracy: 0.8157 - val_loss: 3.5833 - val_categorical_accuracy: 0.3684 - val_top_k_categorical_accuracy: 0.6081\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 2.1725 - categorical_accuracy: 0.5899 - top_k_categorical_accuracy: 0.8175 - val_loss: 2.7598 - val_categorical_accuracy: 0.4945 - val_top_k_categorical_accuracy: 0.7392\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 2.1617 - categorical_accuracy: 0.5926 - top_k_categorical_accuracy: 0.8189 - val_loss: 2.6016 - val_categorical_accuracy: 0.5125 - val_top_k_categorical_accuracy: 0.7572\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2588s 518ms/step - loss: 2.1537 - categorical_accuracy: 0.5936 - top_k_categorical_accuracy: 0.8204 - val_loss: 3.1234 - val_categorical_accuracy: 0.4364 - val_top_k_categorical_accuracy: 0.6923\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2588s 518ms/step - loss: 2.1442 - categorical_accuracy: 0.5952 - top_k_categorical_accuracy: 0.8211 - val_loss: 2.4830 - val_categorical_accuracy: 0.5402 - val_top_k_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.6000000238418579.\n",
      "5000/5000 [==============================] - 2589s 518ms/step - loss: 2.1301 - categorical_accuracy: 0.5983 - top_k_categorical_accuracy: 0.8231 - val_loss: 2.3550 - val_categorical_accuracy: 0.5594 - val_top_k_categorical_accuracy: 0.7962\n",
      "\n",
      "Epoch 00040: val_loss improved from 2.40386 to 2.35500, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.5594.h5\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2590s 518ms/step - loss: 1.6520 - categorical_accuracy: 0.6703 - top_k_categorical_accuracy: 0.8655 - val_loss: 1.6599 - val_categorical_accuracy: 0.6543 - val_top_k_categorical_accuracy: 0.8614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: val_loss improved from 2.35500 to 1.65993, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.6543.h5\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2585s 517ms/step - loss: 1.5076 - categorical_accuracy: 0.6796 - top_k_categorical_accuracy: 0.8727 - val_loss: 1.6858 - val_categorical_accuracy: 0.6405 - val_top_k_categorical_accuracy: 0.8532\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2586s 517ms/step - loss: 1.4809 - categorical_accuracy: 0.6805 - top_k_categorical_accuracy: 0.8738 - val_loss: 1.6562 - val_categorical_accuracy: 0.6412 - val_top_k_categorical_accuracy: 0.8573\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.65993 to 1.65623, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.6412.h5\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 1.4697 - categorical_accuracy: 0.6814 - top_k_categorical_accuracy: 0.8744 - val_loss: 2.1504 - val_categorical_accuracy: 0.5543 - val_top_k_categorical_accuracy: 0.7827\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2586s 517ms/step - loss: 1.4601 - categorical_accuracy: 0.6826 - top_k_categorical_accuracy: 0.8753 - val_loss: 1.7559 - val_categorical_accuracy: 0.6164 - val_top_k_categorical_accuracy: 0.8414\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2584s 517ms/step - loss: 1.4544 - categorical_accuracy: 0.6837 - top_k_categorical_accuracy: 0.8759 - val_loss: 1.9346 - val_categorical_accuracy: 0.5931 - val_top_k_categorical_accuracy: 0.8193\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2581s 516ms/step - loss: 1.4455 - categorical_accuracy: 0.6849 - top_k_categorical_accuracy: 0.8768 - val_loss: 1.7395 - val_categorical_accuracy: 0.6271 - val_top_k_categorical_accuracy: 0.8472\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2580s 516ms/step - loss: 1.4428 - categorical_accuracy: 0.6856 - top_k_categorical_accuracy: 0.8772 - val_loss: 1.7361 - val_categorical_accuracy: 0.6311 - val_top_k_categorical_accuracy: 0.8446\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2581s 516ms/step - loss: 1.4369 - categorical_accuracy: 0.6864 - top_k_categorical_accuracy: 0.8780 - val_loss: 1.9807 - val_categorical_accuracy: 0.5832 - val_top_k_categorical_accuracy: 0.8113\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2585s 517ms/step - loss: 1.4339 - categorical_accuracy: 0.6870 - top_k_categorical_accuracy: 0.8787 - val_loss: 1.7249 - val_categorical_accuracy: 0.6298 - val_top_k_categorical_accuracy: 0.8446\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2584s 517ms/step - loss: 1.4282 - categorical_accuracy: 0.6883 - top_k_categorical_accuracy: 0.8792 - val_loss: 1.7219 - val_categorical_accuracy: 0.6323 - val_top_k_categorical_accuracy: 0.8518\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2586s 517ms/step - loss: 1.4263 - categorical_accuracy: 0.6885 - top_k_categorical_accuracy: 0.8794 - val_loss: 1.6619 - val_categorical_accuracy: 0.6437 - val_top_k_categorical_accuracy: 0.8568\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2586s 517ms/step - loss: 1.4218 - categorical_accuracy: 0.6886 - top_k_categorical_accuracy: 0.8801 - val_loss: 1.5874 - val_categorical_accuracy: 0.6559 - val_top_k_categorical_accuracy: 0.8689\n",
      "\n",
      "Epoch 00053: val_loss improved from 1.65623 to 1.58736, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.6559.h5\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2586s 517ms/step - loss: 1.4188 - categorical_accuracy: 0.6892 - top_k_categorical_accuracy: 0.8804 - val_loss: 1.6541 - val_categorical_accuracy: 0.6439 - val_top_k_categorical_accuracy: 0.8548\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2588s 518ms/step - loss: 1.4135 - categorical_accuracy: 0.6911 - top_k_categorical_accuracy: 0.8810 - val_loss: 1.7537 - val_categorical_accuracy: 0.6289 - val_top_k_categorical_accuracy: 0.8407\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2586s 517ms/step - loss: 1.4123 - categorical_accuracy: 0.6911 - top_k_categorical_accuracy: 0.8811 - val_loss: 1.6168 - val_categorical_accuracy: 0.6500 - val_top_k_categorical_accuracy: 0.8624\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2588s 518ms/step - loss: 1.4107 - categorical_accuracy: 0.6913 - top_k_categorical_accuracy: 0.8817 - val_loss: 1.6347 - val_categorical_accuracy: 0.6504 - val_top_k_categorical_accuracy: 0.8609\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 1.4069 - categorical_accuracy: 0.6921 - top_k_categorical_accuracy: 0.8821 - val_loss: 1.8942 - val_categorical_accuracy: 0.5952 - val_top_k_categorical_accuracy: 0.8234\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 1.4049 - categorical_accuracy: 0.6931 - top_k_categorical_accuracy: 0.8821 - val_loss: 1.5814 - val_categorical_accuracy: 0.6604 - val_top_k_categorical_accuracy: 0.8671\n",
      "\n",
      "Epoch 00059: val_loss improved from 1.58736 to 1.58144, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.6604.h5\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2589s 518ms/step - loss: 1.4005 - categorical_accuracy: 0.6935 - top_k_categorical_accuracy: 0.8829 - val_loss: 1.6459 - val_categorical_accuracy: 0.6498 - val_top_k_categorical_accuracy: 0.8599\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2586s 517ms/step - loss: 1.3980 - categorical_accuracy: 0.6937 - top_k_categorical_accuracy: 0.8832 - val_loss: 1.9123 - val_categorical_accuracy: 0.6021 - val_top_k_categorical_accuracy: 0.8171\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2589s 518ms/step - loss: 1.3969 - categorical_accuracy: 0.6942 - top_k_categorical_accuracy: 0.8831 - val_loss: 1.6376 - val_categorical_accuracy: 0.6516 - val_top_k_categorical_accuracy: 0.8621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2588s 518ms/step - loss: 1.3923 - categorical_accuracy: 0.6952 - top_k_categorical_accuracy: 0.8841 - val_loss: 1.7633 - val_categorical_accuracy: 0.6232 - val_top_k_categorical_accuracy: 0.8418\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 1.3890 - categorical_accuracy: 0.6962 - top_k_categorical_accuracy: 0.8841 - val_loss: 1.7102 - val_categorical_accuracy: 0.6371 - val_top_k_categorical_accuracy: 0.8481\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2589s 518ms/step - loss: 1.3899 - categorical_accuracy: 0.6954 - top_k_categorical_accuracy: 0.8842 - val_loss: 1.7813 - val_categorical_accuracy: 0.6246 - val_top_k_categorical_accuracy: 0.8366\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 1.3872 - categorical_accuracy: 0.6959 - top_k_categorical_accuracy: 0.8843 - val_loss: 1.5688 - val_categorical_accuracy: 0.6604 - val_top_k_categorical_accuracy: 0.8703\n",
      "\n",
      "Epoch 00066: val_loss improved from 1.58144 to 1.56885, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.6604.h5\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 1.3858 - categorical_accuracy: 0.6963 - top_k_categorical_accuracy: 0.8846 - val_loss: 1.7748 - val_categorical_accuracy: 0.6287 - val_top_k_categorical_accuracy: 0.8393\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 1.3816 - categorical_accuracy: 0.6969 - top_k_categorical_accuracy: 0.8853 - val_loss: 1.6734 - val_categorical_accuracy: 0.6449 - val_top_k_categorical_accuracy: 0.8584\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2586s 517ms/step - loss: 1.3790 - categorical_accuracy: 0.6977 - top_k_categorical_accuracy: 0.8855 - val_loss: 1.5814 - val_categorical_accuracy: 0.6607 - val_top_k_categorical_accuracy: 0.8703\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 1.3786 - categorical_accuracy: 0.6981 - top_k_categorical_accuracy: 0.8855 - val_loss: 1.6247 - val_categorical_accuracy: 0.6471 - val_top_k_categorical_accuracy: 0.8622\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2587s 517ms/step - loss: 1.3756 - categorical_accuracy: 0.6979 - top_k_categorical_accuracy: 0.8861 - val_loss: 1.5918 - val_categorical_accuracy: 0.6570 - val_top_k_categorical_accuracy: 0.8674\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2586s 517ms/step - loss: 1.3745 - categorical_accuracy: 0.6983 - top_k_categorical_accuracy: 0.8859 - val_loss: 2.0305 - val_categorical_accuracy: 0.5820 - val_top_k_categorical_accuracy: 0.8040\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2586s 517ms/step - loss: 1.3707 - categorical_accuracy: 0.6998 - top_k_categorical_accuracy: 0.8867 - val_loss: 1.6436 - val_categorical_accuracy: 0.6502 - val_top_k_categorical_accuracy: 0.8584\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2586s 517ms/step - loss: 1.3695 - categorical_accuracy: 0.6997 - top_k_categorical_accuracy: 0.8866 - val_loss: 1.6992 - val_categorical_accuracy: 0.6361 - val_top_k_categorical_accuracy: 0.8545\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2585s 517ms/step - loss: 1.3666 - categorical_accuracy: 0.7001 - top_k_categorical_accuracy: 0.8875 - val_loss: 1.6388 - val_categorical_accuracy: 0.6451 - val_top_k_categorical_accuracy: 0.8616\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2585s 517ms/step - loss: 1.3654 - categorical_accuracy: 0.7003 - top_k_categorical_accuracy: 0.8876 - val_loss: 1.7457 - val_categorical_accuracy: 0.6380 - val_top_k_categorical_accuracy: 0.8426\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2584s 517ms/step - loss: 1.3648 - categorical_accuracy: 0.7006 - top_k_categorical_accuracy: 0.8875 - val_loss: 1.6459 - val_categorical_accuracy: 0.6491 - val_top_k_categorical_accuracy: 0.8633\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2584s 517ms/step - loss: 1.3615 - categorical_accuracy: 0.7013 - top_k_categorical_accuracy: 0.8879 - val_loss: 1.6076 - val_categorical_accuracy: 0.6608 - val_top_k_categorical_accuracy: 0.8611\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2582s 516ms/step - loss: 1.3610 - categorical_accuracy: 0.7015 - top_k_categorical_accuracy: 0.8880 - val_loss: 1.9689 - val_categorical_accuracy: 0.5951 - val_top_k_categorical_accuracy: 0.8155\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.06000000238418579.\n",
      "5000/5000 [==============================] - 2579s 516ms/step - loss: 1.3587 - categorical_accuracy: 0.7019 - top_k_categorical_accuracy: 0.8883 - val_loss: 1.6268 - val_categorical_accuracy: 0.6532 - val_top_k_categorical_accuracy: 0.8632\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.006000000238418579.\n",
      "5000/5000 [==============================] - 2579s 516ms/step - loss: 1.2593 - categorical_accuracy: 0.7247 - top_k_categorical_accuracy: 0.8984 - val_loss: 1.4965 - val_categorical_accuracy: 0.6829 - val_top_k_categorical_accuracy: 0.8724\n",
      "\n",
      "Epoch 00081: val_loss improved from 1.56885 to 1.49646, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.6829.h5\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2581s 516ms/step - loss: 1.2321 - categorical_accuracy: 0.7282 - top_k_categorical_accuracy: 0.9006 - val_loss: 1.4906 - val_categorical_accuracy: 0.6771 - val_top_k_categorical_accuracy: 0.8744\n",
      "\n",
      "Epoch 00082: val_loss improved from 1.49646 to 1.49064, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.6771.h5\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2583s 517ms/step - loss: 1.2197 - categorical_accuracy: 0.7293 - top_k_categorical_accuracy: 0.9013 - val_loss: 1.4526 - val_categorical_accuracy: 0.6771 - val_top_k_categorical_accuracy: 0.8801\n",
      "\n",
      "Epoch 00083: val_loss improved from 1.49064 to 1.45256, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.6771.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2581s 516ms/step - loss: 1.2110 - categorical_accuracy: 0.7301 - top_k_categorical_accuracy: 0.9016 - val_loss: 1.4844 - val_categorical_accuracy: 0.6728 - val_top_k_categorical_accuracy: 0.8759\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2582s 516ms/step - loss: 1.2040 - categorical_accuracy: 0.7304 - top_k_categorical_accuracy: 0.9018 - val_loss: 1.4722 - val_categorical_accuracy: 0.6804 - val_top_k_categorical_accuracy: 0.8764\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2581s 516ms/step - loss: 1.2020 - categorical_accuracy: 0.7299 - top_k_categorical_accuracy: 0.9020 - val_loss: 1.4750 - val_categorical_accuracy: 0.6725 - val_top_k_categorical_accuracy: 0.8750\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2583s 517ms/step - loss: 1.1947 - categorical_accuracy: 0.7306 - top_k_categorical_accuracy: 0.9022 - val_loss: 1.4586 - val_categorical_accuracy: 0.6794 - val_top_k_categorical_accuracy: 0.8746\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2573s 515ms/step - loss: 1.1950 - categorical_accuracy: 0.7303 - top_k_categorical_accuracy: 0.9020 - val_loss: 1.4383 - val_categorical_accuracy: 0.6864 - val_top_k_categorical_accuracy: 0.8754\n",
      "\n",
      "Epoch 00088: val_loss improved from 1.45256 to 1.43831, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.6864.h5\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2566s 513ms/step - loss: 1.1913 - categorical_accuracy: 0.7308 - top_k_categorical_accuracy: 0.9021 - val_loss: 1.4021 - val_categorical_accuracy: 0.6863 - val_top_k_categorical_accuracy: 0.8842\n",
      "\n",
      "Epoch 00089: val_loss improved from 1.43831 to 1.40206, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.6863.h5\n",
      "Epoch 90/200\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2566s 513ms/step - loss: 1.1888 - categorical_accuracy: 0.7305 - top_k_categorical_accuracy: 0.9023 - val_loss: 1.4331 - val_categorical_accuracy: 0.6798 - val_top_k_categorical_accuracy: 0.8831\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/200\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2571s 514ms/step - loss: 1.1851 - categorical_accuracy: 0.7308 - top_k_categorical_accuracy: 0.9028 - val_loss: 1.4791 - val_categorical_accuracy: 0.6722 - val_top_k_categorical_accuracy: 0.8752\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/200\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2576s 515ms/step - loss: 1.1860 - categorical_accuracy: 0.7303 - top_k_categorical_accuracy: 0.9025 - val_loss: 1.4307 - val_categorical_accuracy: 0.6876 - val_top_k_categorical_accuracy: 0.8751\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/200\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2577s 515ms/step - loss: 1.1825 - categorical_accuracy: 0.7307 - top_k_categorical_accuracy: 0.9028 - val_loss: 1.4685 - val_categorical_accuracy: 0.6711 - val_top_k_categorical_accuracy: 0.8785\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/200\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2610s 522ms/step - loss: 1.1845 - categorical_accuracy: 0.7306 - top_k_categorical_accuracy: 0.9024 - val_loss: 1.4779 - val_categorical_accuracy: 0.6697 - val_top_k_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/200\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2605s 521ms/step - loss: 1.1811 - categorical_accuracy: 0.7310 - top_k_categorical_accuracy: 0.9024 - val_loss: 1.4997 - val_categorical_accuracy: 0.6674 - val_top_k_categorical_accuracy: 0.8687\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/200\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2575s 515ms/step - loss: 1.1808 - categorical_accuracy: 0.7307 - top_k_categorical_accuracy: 0.9025 - val_loss: 1.4384 - val_categorical_accuracy: 0.6799 - val_top_k_categorical_accuracy: 0.8768\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/200\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2607s 521ms/step - loss: 1.1804 - categorical_accuracy: 0.7303 - top_k_categorical_accuracy: 0.9025 - val_loss: 1.4364 - val_categorical_accuracy: 0.6795 - val_top_k_categorical_accuracy: 0.8765\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/200\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2612s 522ms/step - loss: 1.1777 - categorical_accuracy: 0.7311 - top_k_categorical_accuracy: 0.9025 - val_loss: 1.4157 - val_categorical_accuracy: 0.6803 - val_top_k_categorical_accuracy: 0.8798\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/200\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2606s 521ms/step - loss: 1.1753 - categorical_accuracy: 0.7315 - top_k_categorical_accuracy: 0.9027 - val_loss: 1.4725 - val_categorical_accuracy: 0.6751 - val_top_k_categorical_accuracy: 0.8708\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/200\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2599s 520ms/step - loss: 1.1769 - categorical_accuracy: 0.7309 - top_k_categorical_accuracy: 0.9028 - val_loss: 1.4237 - val_categorical_accuracy: 0.6785 - val_top_k_categorical_accuracy: 0.8839\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2558s 512ms/step - loss: 1.1751 - categorical_accuracy: 0.7309 - top_k_categorical_accuracy: 0.9030 - val_loss: 1.4401 - val_categorical_accuracy: 0.6806 - val_top_k_categorical_accuracy: 0.8772\n",
      "\n",
      "Epoch 00101: val_loss did not improve\n",
      "Epoch 102/200\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2596s 519ms/step - loss: 1.1752 - categorical_accuracy: 0.7310 - top_k_categorical_accuracy: 0.9027 - val_loss: 1.3972 - val_categorical_accuracy: 0.6846 - val_top_k_categorical_accuracy: 0.8855\n",
      "\n",
      "Epoch 00102: val_loss improved from 1.40206 to 1.39724, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.6846.h5\n",
      "Epoch 103/200\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2599s 520ms/step - loss: 1.1738 - categorical_accuracy: 0.7317 - top_k_categorical_accuracy: 0.9026 - val_loss: 1.4431 - val_categorical_accuracy: 0.6770 - val_top_k_categorical_accuracy: 0.8755\n",
      "\n",
      "Epoch 00103: val_loss did not improve\n",
      "Epoch 104/200\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2599s 520ms/step - loss: 1.1736 - categorical_accuracy: 0.7311 - top_k_categorical_accuracy: 0.9031 - val_loss: 1.4312 - val_categorical_accuracy: 0.6800 - val_top_k_categorical_accuracy: 0.8773\n",
      "\n",
      "Epoch 00104: val_loss did not improve\n",
      "Epoch 105/200\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 2588s 518ms/step - loss: 1.1721 - categorical_accuracy: 0.7314 - top_k_categorical_accuracy: 0.9032 - val_loss: 1.4273 - val_categorical_accuracy: 0.6789 - val_top_k_categorical_accuracy: 0.8772\n",
      "\n",
      "Epoch 00105: val_loss did not improve\n",
      "Epoch 106/200\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2573s 515ms/step - loss: 1.1711 - categorical_accuracy: 0.7316 - top_k_categorical_accuracy: 0.9031 - val_loss: 1.5073 - val_categorical_accuracy: 0.6683 - val_top_k_categorical_accuracy: 0.8639\n",
      "\n",
      "Epoch 00106: val_loss did not improve\n",
      "Epoch 107/200\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2575s 515ms/step - loss: 1.1725 - categorical_accuracy: 0.7309 - top_k_categorical_accuracy: 0.9028 - val_loss: 1.3980 - val_categorical_accuracy: 0.6898 - val_top_k_categorical_accuracy: 0.8798\n",
      "\n",
      "Epoch 00107: val_loss did not improve\n",
      "Epoch 108/200\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2575s 515ms/step - loss: 1.1686 - categorical_accuracy: 0.7316 - top_k_categorical_accuracy: 0.9033 - val_loss: 1.4721 - val_categorical_accuracy: 0.6704 - val_top_k_categorical_accuracy: 0.8742\n",
      "\n",
      "Epoch 00108: val_loss did not improve\n",
      "Epoch 109/200\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2574s 515ms/step - loss: 1.1718 - categorical_accuracy: 0.7310 - top_k_categorical_accuracy: 0.9029 - val_loss: 1.3993 - val_categorical_accuracy: 0.6836 - val_top_k_categorical_accuracy: 0.8836\n",
      "\n",
      "Epoch 00109: val_loss did not improve\n",
      "Epoch 110/200\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2578s 516ms/step - loss: 1.1683 - categorical_accuracy: 0.7314 - top_k_categorical_accuracy: 0.9034 - val_loss: 1.4512 - val_categorical_accuracy: 0.6774 - val_top_k_categorical_accuracy: 0.8766\n",
      "\n",
      "Epoch 00110: val_loss did not improve\n",
      "Epoch 111/200\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2575s 515ms/step - loss: 1.1675 - categorical_accuracy: 0.7316 - top_k_categorical_accuracy: 0.9035 - val_loss: 1.3718 - val_categorical_accuracy: 0.6855 - val_top_k_categorical_accuracy: 0.8804\n",
      "\n",
      "Epoch 00111: val_loss improved from 1.39724 to 1.37183, saving model to /home/user/project/checkpoints/se_resnet34/weights-0.6855.h5\n",
      "Epoch 112/200\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2576s 515ms/step - loss: 1.1684 - categorical_accuracy: 0.7314 - top_k_categorical_accuracy: 0.9031 - val_loss: 1.4311 - val_categorical_accuracy: 0.6829 - val_top_k_categorical_accuracy: 0.8804\n",
      "\n",
      "Epoch 00112: val_loss did not improve\n",
      "Epoch 113/200\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2574s 515ms/step - loss: 1.1667 - categorical_accuracy: 0.7315 - top_k_categorical_accuracy: 0.9034 - val_loss: 1.4991 - val_categorical_accuracy: 0.6661 - val_top_k_categorical_accuracy: 0.8677\n",
      "\n",
      "Epoch 00113: val_loss did not improve\n",
      "Epoch 114/200\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2575s 515ms/step - loss: 1.1682 - categorical_accuracy: 0.7317 - top_k_categorical_accuracy: 0.9031 - val_loss: 1.5517 - val_categorical_accuracy: 0.6568 - val_top_k_categorical_accuracy: 0.8620\n",
      "\n",
      "Epoch 00114: val_loss did not improve\n",
      "Epoch 115/200\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2574s 515ms/step - loss: 1.1643 - categorical_accuracy: 0.7319 - top_k_categorical_accuracy: 0.9040 - val_loss: 1.4764 - val_categorical_accuracy: 0.6688 - val_top_k_categorical_accuracy: 0.8736\n",
      "\n",
      "Epoch 00115: val_loss did not improve\n",
      "Epoch 116/200\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2576s 515ms/step - loss: 1.1654 - categorical_accuracy: 0.7314 - top_k_categorical_accuracy: 0.9037 - val_loss: 1.4292 - val_categorical_accuracy: 0.6802 - val_top_k_categorical_accuracy: 0.8779\n",
      "\n",
      "Epoch 00116: val_loss did not improve\n",
      "Epoch 117/200\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2575s 515ms/step - loss: 1.1631 - categorical_accuracy: 0.7321 - top_k_categorical_accuracy: 0.9040 - val_loss: 1.4503 - val_categorical_accuracy: 0.6735 - val_top_k_categorical_accuracy: 0.8733\n",
      "\n",
      "Epoch 00117: val_loss did not improve\n",
      "Epoch 118/200\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2577s 515ms/step - loss: 1.1632 - categorical_accuracy: 0.7322 - top_k_categorical_accuracy: 0.9037 - val_loss: 1.4101 - val_categorical_accuracy: 0.6785 - val_top_k_categorical_accuracy: 0.8828\n",
      "\n",
      "Epoch 00118: val_loss did not improve\n",
      "Epoch 119/200\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2575s 515ms/step - loss: 1.1633 - categorical_accuracy: 0.7325 - top_k_categorical_accuracy: 0.9038 - val_loss: 1.4965 - val_categorical_accuracy: 0.6652 - val_top_k_categorical_accuracy: 0.8706\n",
      "\n",
      "Epoch 00119: val_loss did not improve\n",
      "Epoch 120/200\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.006000000052154064.\n",
      "5000/5000 [==============================] - 2576s 515ms/step - loss: 1.1632 - categorical_accuracy: 0.7320 - top_k_categorical_accuracy: 0.9038 - val_loss: 1.4726 - val_categorical_accuracy: 0.6719 - val_top_k_categorical_accuracy: 0.8705\n",
      "\n",
      "Epoch 00120: val_loss did not improve\n",
      "Epoch 121/200\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.0006000000052154064.\n",
      "5000/5000 [==============================] - 2567s 513ms/step - loss: 1.1438 - categorical_accuracy: 0.7369 - top_k_categorical_accuracy: 0.9061 - val_loss: 1.3835 - val_categorical_accuracy: 0.6888 - val_top_k_categorical_accuracy: 0.8839\n",
      "\n",
      "Epoch 00121: val_loss did not improve\n",
      "Epoch 122/200\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      "5000/5000 [==============================] - 2562s 512ms/step - loss: 1.1413 - categorical_accuracy: 0.7377 - top_k_categorical_accuracy: 0.9061 - val_loss: 1.3832 - val_categorical_accuracy: 0.6902 - val_top_k_categorical_accuracy: 0.8859\n",
      "\n",
      "Epoch 00122: val_loss did not improve\n",
      "Epoch 123/200\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      "5000/5000 [==============================] - 2563s 513ms/step - loss: 1.1399 - categorical_accuracy: 0.7378 - top_k_categorical_accuracy: 0.9063 - val_loss: 1.4126 - val_categorical_accuracy: 0.6803 - val_top_k_categorical_accuracy: 0.8779\n",
      "\n",
      "Epoch 00123: val_loss did not improve\n",
      "Epoch 124/200\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      "5000/5000 [==============================] - 2559s 512ms/step - loss: 1.1378 - categorical_accuracy: 0.7380 - top_k_categorical_accuracy: 0.9066 - val_loss: 1.4260 - val_categorical_accuracy: 0.6851 - val_top_k_categorical_accuracy: 0.8758\n",
      "\n",
      "Epoch 00124: val_loss did not improve\n",
      "Epoch 125/200\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      "5000/5000 [==============================] - 2561s 512ms/step - loss: 1.1385 - categorical_accuracy: 0.7381 - top_k_categorical_accuracy: 0.9065 - val_loss: 1.3889 - val_categorical_accuracy: 0.6830 - val_top_k_categorical_accuracy: 0.8845\n",
      "\n",
      "Epoch 00125: val_loss did not improve\n",
      "Epoch 126/200\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      "5000/5000 [==============================] - 2561s 512ms/step - loss: 1.1388 - categorical_accuracy: 0.7377 - top_k_categorical_accuracy: 0.9065 - val_loss: 1.4110 - val_categorical_accuracy: 0.6830 - val_top_k_categorical_accuracy: 0.8806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00126: val_loss did not improve\n",
      "Epoch 127/200\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      "5000/5000 [==============================] - 2573s 515ms/step - loss: 1.1371 - categorical_accuracy: 0.7385 - top_k_categorical_accuracy: 0.9064 - val_loss: 1.3871 - val_categorical_accuracy: 0.6850 - val_top_k_categorical_accuracy: 0.8804\n",
      "\n",
      "Epoch 00127: val_loss did not improve\n",
      "Epoch 128/200\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      "5000/5000 [==============================] - 2593s 519ms/step - loss: 1.1387 - categorical_accuracy: 0.7379 - top_k_categorical_accuracy: 0.9063 - val_loss: 1.4151 - val_categorical_accuracy: 0.6820 - val_top_k_categorical_accuracy: 0.8810\n",
      "\n",
      "Epoch 00128: val_loss did not improve\n",
      "Epoch 129/200\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      "5000/5000 [==============================] - 2614s 523ms/step - loss: 1.1364 - categorical_accuracy: 0.7382 - top_k_categorical_accuracy: 0.9067 - val_loss: 1.3786 - val_categorical_accuracy: 0.6908 - val_top_k_categorical_accuracy: 0.8816\n",
      "\n",
      "Epoch 00129: val_loss did not improve\n",
      "Epoch 130/200\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      "5000/5000 [==============================] - 2625s 525ms/step - loss: 1.1381 - categorical_accuracy: 0.7381 - top_k_categorical_accuracy: 0.9063 - val_loss: 1.3859 - val_categorical_accuracy: 0.6888 - val_top_k_categorical_accuracy: 0.8842\n",
      "\n",
      "Epoch 00130: val_loss did not improve\n",
      "Epoch 131/200\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      "5000/5000 [==============================] - 2633s 527ms/step - loss: 1.1356 - categorical_accuracy: 0.7384 - top_k_categorical_accuracy: 0.9070 - val_loss: 1.4017 - val_categorical_accuracy: 0.6820 - val_top_k_categorical_accuracy: 0.8786\n",
      "\n",
      "Epoch 00131: val_loss did not improve\n",
      "Epoch 132/200\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      "5000/5000 [==============================] - 2567s 513ms/step - loss: 1.1372 - categorical_accuracy: 0.7381 - top_k_categorical_accuracy: 0.9065 - val_loss: 1.4251 - val_categorical_accuracy: 0.6842 - val_top_k_categorical_accuracy: 0.8803\n",
      "\n",
      "Epoch 00132: val_loss did not improve\n",
      "Epoch 133/200\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      "5000/5000 [==============================] - 2562s 512ms/step - loss: 1.1350 - categorical_accuracy: 0.7386 - top_k_categorical_accuracy: 0.9066 - val_loss: 1.3855 - val_categorical_accuracy: 0.6858 - val_top_k_categorical_accuracy: 0.8815\n",
      "\n",
      "Epoch 00133: val_loss did not improve\n",
      "Epoch 134/200\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      "5000/5000 [==============================] - 2606s 521ms/step - loss: 1.1344 - categorical_accuracy: 0.7388 - top_k_categorical_accuracy: 0.9067 - val_loss: 1.3881 - val_categorical_accuracy: 0.6866 - val_top_k_categorical_accuracy: 0.8849\n",
      "\n",
      "Epoch 00134: val_loss did not improve\n",
      "Epoch 135/200\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.0006000000284984708.\n",
      " 409/5000 [=>............................] - ETA: 36:18 - loss: 1.1451 - categorical_accuracy: 0.7362 - top_k_categorical_accuracy: 0.9064"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1927:\n",
      "Process Process-1924:\n",
      "Process Process-1873:\n",
      "Process Process-1929:\n",
      "Process Process-1930:\n",
      "Process Process-1880:\n",
      "Process Process-1928:\n",
      "Process Process-1878:\n",
      "Process Process-1921:\n",
      "Process Process-1882:\n",
      "Process Process-1884:\n",
      "Process Process-1925:\n",
      "Process Process-1879:\n",
      "Process Process-1922:\n",
      "Process Process-1877:\n",
      "Process Process-1883:\n",
      "Process Process-1926:\n",
      "Process Process-1932:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-1875:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-1874:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-1923:\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-1881:\n",
      "Process Process-1876:\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-1931:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"../utils/dataset.py\", line 33, in __getitem__\n",
      "    'image': self.imread(self.samples[i][0]),\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"../utils/dataset.py\", line 28, in imread\n",
      "    return cv2.imread(path)[..., ::-1]\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"../utils/dataset.py\", line 38, in __getitem__\n",
      "    sample = self.transform(**sample)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"../utils/dataset.py\", line 38, in __getitem__\n",
      "    sample = self.transform(**sample)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 110, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/albumentations/core/composition.py\", line 58, in __call__\n",
      "    data = self.run_transforms_if_needed(need_to_run, data)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 341, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/albumentations/core/composition.py\", line 70, in run_transforms_if_needed\n",
      "    data = t(**data)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 190, in reduce_storage\n",
      "    fd, size = storage._share_fd_()\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/albumentations/core/composition.py\", line 129, in __call__\n",
      "    data = t(**data)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/albumentations/core/transforms_interface.py\", line 27, in __call__\n",
      "    res[key] = target_function(arg, **dict(params, **target_dependencies))\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/albumentations/augmentations/transforms.py\", line 777, in apply\n",
      "    return F.random_contrast(img, alpha)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/albumentations/augmentations/functional.py\", line 186, in wrapped_function\n",
      "    return clip(func(img, *args, **kwargs), dtype, maxval)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/albumentations/augmentations/functional.py\", line 178, in clip\n",
      "    return np.clip(img, 0, maxval).astype(dtype)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/albumentations/core/composition.py\", line 58, in __call__\n",
      "    data = self.run_transforms_if_needed(need_to_run, data)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"../utils/dataset.py\", line 33, in __getitem__\n",
      "    'image': self.imread(self.samples[i][0]),\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/albumentations/core/composition.py\", line 70, in run_transforms_if_needed\n",
      "    data = t(**data)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"../utils/dataset.py\", line 28, in imread\n",
      "    return cv2.imread(path)[..., ::-1]\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/albumentations/core/transforms_interface.py\", line 27, in __call__\n",
      "    res[key] = target_function(arg, **dict(params, **target_dependencies))\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/albumentations/augmentations/transforms.py\", line 209, in apply\n",
      "    return F.smallest_max_size(img, max_size=self.max_size, interpolation=interpolation)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/albumentations/augmentations/functional.py\", line 277, in smallest_max_size\n",
      "    return _func_max_size(img, max_size, interpolation, min)\n",
      "  File \"/opt/anaconda/anaconda3/lib/python3.6/site-packages/albumentations/augmentations/functional.py\", line 268, in _func_max_size\n",
      "    img = cv2.resize(img, out_size, interpolation=interpolation)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dddff749676d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_gen,\n",
    "                    steps_per_epoch=5000, \n",
    "                    initial_epoch=0,\n",
    "                    epochs=200, \n",
    "                    validation_data=valid_gen, \n",
    "                    validation_steps=10000, \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('../../checkpoints/se_resnet18/weights_ep310.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
